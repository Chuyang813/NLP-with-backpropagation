{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "b7e6a263",
   "metadata": {
    "id": "b7e6a263"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "eKE_KANU7_sq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eKE_KANU7_sq",
    "outputId": "946cd1ab-22bd-4efa-c734-faab97e6f40d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "9a5cbc46",
   "metadata": {
    "id": "9a5cbc46"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df_train = pd.read_csv('train.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "id": "e6f3afc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_train = df_train.sample(n=int(len(df_train) * 0.7))\n",
    "df_train_test = df_train[~df_train.index.isin(df_train_train.index)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "4a5e1994",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = list(df_train_train['description_x'].values)\n",
    "train_x_1 = list(df_train_train['description_y'].values)\n",
    "\n",
    "test_x = list(df_train_test['description_x'].values)\n",
    "test_x_1 = list(df_train_test['description_y'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "Q5G-vmpD21Bj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "id": "Q5G-vmpD21Bj",
    "outputId": "f053f294-9aad-4097-f75e-aaed70dd050e"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "####Train tf-idf model and get  vectorized representation of sentences####\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5)#, stop_words='english')\n",
    "tfidf.fit(train_x)\n",
    "train_X = tfidf.transform(train_x).toarray()\n",
    "test_X = tfidf.transform(test_x).toarray()\n",
    "train_X_1 = tfidf.transform(train_x_1).toarray()\n",
    "test_X_1 = tfidf.transform(test_x_1).toarray()\n",
    "\n",
    "\n",
    "label_dict = {'True':1,'False':0}\n",
    "label_train = []\n",
    "label_test = []\n",
    "df_train_label = np.array(df_train_train['same_security'],dtype=str)\n",
    "df_test_label = np.array(df_train_test['same_security'],dtype=str)\n",
    "for i in range(len(df_train_label)):\n",
    "    if df_train_label[i] == 'True':\n",
    "        label_train.append(1)\n",
    "    else:\n",
    "        label_train.append(0)\n",
    "for i in range(len(df_test_label)):\n",
    "    if df_test_label[i] == 'True':\n",
    "        label_test.append(1)\n",
    "    else:\n",
    "        label_test.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "662e082b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = np.concatenate([train_X,train_X_1],axis=1)\n",
    "test_X = np.concatenate([test_X,test_X_1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "8846e380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert true and false to 1 and 0\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(np.array(label_train).reshape(-1, 1))\n",
    "train_Y = enc.transform(np.array(label_train).reshape(-1, 1)).toarray()\n",
    "test_Y = enc.transform(np.array(label_test).reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "07ed55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = torch.tensor(train_X,dtype=torch.float32)\n",
    "train_Y = torch.tensor(train_Y,dtype=torch.float32)\n",
    "test_X = torch.tensor(test_X,dtype=torch.float32)\n",
    "test_Y = torch.tensor(test_Y,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "cdb5937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of val_x: torch.Size([1499, 492])\n",
      "Shape of test_x: torch.Size([643, 492])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of val_x:\",train_X.shape)\n",
    "print(\"Shape of test_x:\",test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "6291c647",
   "metadata": {
    "id": "6291c647"
   },
   "outputs": [],
   "source": [
    "def evaluator(y_test, y_pred):\n",
    "    ####################################################################################################\n",
    "    # enter code here to implement the evaluation metrics including confusion matrix, accuracy, precision and recall\n",
    "    # you can only use Numpy or Pytorch to implement the metrics\n",
    "    confusion_matrix = np.zeros( (y_test.shape[1], y_test.shape[1]) )  \n",
    "    for index, elem in enumerate(y_test):\n",
    "        r, c = np.argmax(elem), y_pred[index]\n",
    "        confusion_matrix[r][c] += 1\n",
    "    accuracy = np.sum(np.argmax(y_test, axis=1) == y_pred) /  y_test.shape[0]\n",
    "    \n",
    "  \n",
    "    #true positive\n",
    "    TP = np.sum(np.logical_and(np.equal(np.argmax(y_test, axis=1),0),np.equal(y_pred,0)))\n",
    "    #false positive\n",
    "    FP = np.sum(np.logical_and(np.not_equal(np.argmax(y_test, axis=1),0),np.equal(y_pred,0)))\n",
    "    \n",
    "    #false negative\n",
    "    FN = np.sum(np.logical_and(np.equal(np.argmax(y_test, axis=1),0),np.not_equal(y_pred,0)))\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    F1_Score = 2*precision*recall/(precision+recall)\n",
    "    return confusion_matrix,accuracy,precision,recall,F1_Score\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4a6b06",
   "metadata": {
    "id": "4e4a6b06"
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "a0d3fc2d",
   "metadata": {
    "id": "a0d3fc2d"
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    def __init__(self, learning_rate, n_iters, batch_size, hidden_size, device, dtype=torch.float32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.history = {}\n",
    "        self.history['train_acc'], self.history['val_acc'], self.history['loss'] = [], [], []\n",
    "    \n",
    "    # 5. activation function\n",
    "    def sigmoid(self, x):\n",
    "        ####################################################################################################\n",
    "        # enter code here to implement the activation function\n",
    "        return 1/(1 + torch.exp(-x))\n",
    "        \n",
    "        ####################################################################################################\n",
    "\n",
    "    def train(self, x, y, x_val, y_val, verbose=1):\n",
    "        n_train = x.shape[0]\n",
    "        n_val = x_val.shape[0]\n",
    "        input_size = x.shape[1]\n",
    "        num_classes = y.shape[1]\n",
    "\n",
    "        # weight initialization\n",
    "        self.W1 = torch.randn(input_size, self.hidden_size, dtype=self.dtype, device=self.device) * 0.01\n",
    "        self.W2 = torch.randn(self.hidden_size, num_classes, dtype=self.dtype, device=self.device) * 0.01\n",
    "\n",
    "        # TODO: train the weights with the input data and labels\n",
    "        for i in range(self.n_iters):\n",
    "            loss = 0\n",
    "            data = getBatch(x, y, self.batch_size)\n",
    "            for x_batch, y_batch in data:\n",
    "                \n",
    "                # 1. forward\n",
    "                ####################################################################################################\n",
    "                # enter code here to calculate the hidden layer output and output layer output\n",
    "\n",
    "                hidden = self.sigmoid(x_batch.cuda().mm(self.W1))\n",
    "                output = self.sigmoid(hidden.cuda().mm(self.W2))\n",
    "                \n",
    "                ####################################################################################################\n",
    "\n",
    "                # 2. error and loss\n",
    "                ####################################################################################################\n",
    "                # enter code here to calculate the output error, MSE loss, delta output and delta hidden\n",
    "                \n",
    "                output_error = y_batch.cuda() - output\n",
    "                loss += output_error.pow(2).sum().item()\n",
    "                delta_output = output_error * output * (1 - output)\n",
    "                delta_hidden = delta_output.mm(self.W2.t())* hidden*(1-hidden)\n",
    "                ####################################################################################################\n",
    "\n",
    "                # 3. backward\n",
    "                ####################################################################################################\n",
    "                # enter code here to calculate delta weights and update the weights\n",
    "                delta_W2 = torch.mm(hidden.t(), delta_output) \n",
    "                delta_W1 = torch.mm(x_batch.cuda().t(), delta_hidden)\n",
    "                self.W2 += delta_W2 * self.learning_rate\n",
    "                self.W1 += delta_W1 * self.learning_rate\n",
    "                \n",
    "                ####################################################################################################\n",
    "\n",
    "            # calculate the accuracy and save the training history\n",
    "            \n",
    "            y_pred = self.predict(x)\n",
    "            train_acc = torch.sum(torch.argmax(y.cuda(), dim=1) == y_pred) / n_train\n",
    "            self.history['train_acc'].append(train_acc.cpu())\n",
    "            self.history['loss'].append(loss)\n",
    "            \n",
    "            y_pred = self.predict(x_val)\n",
    "            val_acc = torch.sum(torch.argmax(y_val.cuda(), dim=1) == y_pred) / n_val\n",
    "            self.history['val_acc'].append(val_acc.cpu())\n",
    "            if verbose:\n",
    "                print('epoch %d, loss %.4f, train acc %.3f, validation acc %.3f'\n",
    "                  % (i + 1, loss, train_acc, val_acc))\n",
    "        \n",
    "\n",
    "    \n",
    "    # 4. predict function \n",
    "    def predict(self, x):\n",
    "        ####################################################################################################\n",
    "        # enter code here to implement the predict function\n",
    "        # TODO: use the trained weights to predict labels and return the predicted labels\n",
    "        # remember to use torch.argmax() to return the true labels\n",
    "        hidden =  self.sigmoid(x.cuda().mm(self.W1))\n",
    "        output =  self.sigmoid(hidden.mm(self.W2))\n",
    "        y_pred = torch.argmax(output, dim=1)\n",
    "        \n",
    "        ####################################################################################################\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "def getBatch(x, y, batch_size):\n",
    "    n_epoch = x.shape[0] // batch_size\n",
    "    for i in range(n_epoch):\n",
    "        x_batch = x[i * batch_size : (i+1) * batch_size]\n",
    "        y_batch = y[i * batch_size : (i+1) * batch_size]\n",
    "      \n",
    "        yield x_batch, y_batch\n",
    "    x_batch = x[(i+1) * batch_size:]\n",
    "    y_batch = y[(i+1) * batch_size:]   \n",
    "\n",
    "    yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "74e9819c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74e9819c",
    "outputId": "c6b1745f-4e19-4f83-afb7-c22f07062738",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 577.6290, train acc 0.755, validation acc 0.750\n",
      "epoch 2, loss 555.4680, train acc 0.755, validation acc 0.750\n",
      "epoch 3, loss 555.1136, train acc 0.755, validation acc 0.750\n",
      "epoch 4, loss 554.7673, train acc 0.755, validation acc 0.750\n",
      "epoch 5, loss 554.4202, train acc 0.755, validation acc 0.750\n",
      "epoch 6, loss 554.0721, train acc 0.755, validation acc 0.750\n",
      "epoch 7, loss 553.7226, train acc 0.755, validation acc 0.750\n",
      "epoch 8, loss 553.3716, train acc 0.755, validation acc 0.750\n",
      "epoch 9, loss 553.0188, train acc 0.755, validation acc 0.750\n",
      "epoch 10, loss 552.6639, train acc 0.755, validation acc 0.750\n",
      "epoch 11, loss 552.3066, train acc 0.755, validation acc 0.750\n",
      "epoch 12, loss 551.9465, train acc 0.755, validation acc 0.750\n",
      "epoch 13, loss 551.5831, train acc 0.755, validation acc 0.750\n",
      "epoch 14, loss 551.2160, train acc 0.755, validation acc 0.750\n",
      "epoch 15, loss 550.8446, train acc 0.755, validation acc 0.750\n",
      "epoch 16, loss 550.4683, train acc 0.755, validation acc 0.750\n",
      "epoch 17, loss 550.0863, train acc 0.755, validation acc 0.750\n",
      "epoch 18, loss 549.6978, train acc 0.755, validation acc 0.750\n",
      "epoch 19, loss 549.3017, train acc 0.755, validation acc 0.750\n",
      "epoch 20, loss 548.8969, train acc 0.755, validation acc 0.750\n",
      "epoch 21, loss 548.4822, train acc 0.755, validation acc 0.750\n",
      "epoch 22, loss 548.0560, train acc 0.755, validation acc 0.750\n",
      "epoch 23, loss 547.6166, train acc 0.755, validation acc 0.750\n",
      "epoch 24, loss 547.1619, train acc 0.755, validation acc 0.750\n",
      "epoch 25, loss 546.6898, train acc 0.755, validation acc 0.750\n",
      "epoch 26, loss 546.1975, train acc 0.755, validation acc 0.750\n",
      "epoch 27, loss 545.6820, train acc 0.755, validation acc 0.750\n",
      "epoch 28, loss 545.1396, train acc 0.755, validation acc 0.750\n",
      "epoch 29, loss 544.5665, train acc 0.755, validation acc 0.750\n",
      "epoch 30, loss 543.9579, train acc 0.755, validation acc 0.750\n",
      "epoch 31, loss 543.3083, train acc 0.755, validation acc 0.750\n",
      "epoch 32, loss 542.6116, train acc 0.755, validation acc 0.750\n",
      "epoch 33, loss 541.8606, train acc 0.755, validation acc 0.750\n",
      "epoch 34, loss 541.0471, train acc 0.755, validation acc 0.750\n",
      "epoch 35, loss 540.1616, train acc 0.755, validation acc 0.750\n",
      "epoch 36, loss 539.1930, train acc 0.755, validation acc 0.750\n",
      "epoch 37, loss 538.1289, train acc 0.755, validation acc 0.750\n",
      "epoch 38, loss 536.9547, train acc 0.755, validation acc 0.750\n",
      "epoch 39, loss 535.6538, train acc 0.755, validation acc 0.750\n",
      "epoch 40, loss 534.2071, train acc 0.755, validation acc 0.750\n",
      "epoch 41, loss 532.5927, train acc 0.755, validation acc 0.750\n",
      "epoch 42, loss 530.7857, train acc 0.755, validation acc 0.750\n",
      "epoch 43, loss 528.7575, train acc 0.755, validation acc 0.750\n",
      "epoch 44, loss 526.4760, train acc 0.755, validation acc 0.750\n",
      "epoch 45, loss 523.9046, train acc 0.755, validation acc 0.750\n",
      "epoch 46, loss 521.0027, train acc 0.755, validation acc 0.750\n",
      "epoch 47, loss 517.7252, train acc 0.755, validation acc 0.750\n",
      "epoch 48, loss 514.0233, train acc 0.755, validation acc 0.750\n",
      "epoch 49, loss 509.8447, train acc 0.755, validation acc 0.750\n",
      "epoch 50, loss 505.1355, train acc 0.755, validation acc 0.750\n",
      "epoch 51, loss 499.8428, train acc 0.755, validation acc 0.750\n",
      "epoch 52, loss 493.9180, train acc 0.755, validation acc 0.750\n",
      "epoch 53, loss 487.3224, train acc 0.755, validation acc 0.750\n",
      "epoch 54, loss 480.0335, train acc 0.755, validation acc 0.750\n",
      "epoch 55, loss 472.0526, train acc 0.755, validation acc 0.750\n",
      "epoch 56, loss 463.4125, train acc 0.755, validation acc 0.750\n",
      "epoch 57, loss 454.1830, train acc 0.757, validation acc 0.751\n",
      "epoch 58, loss 444.4727, train acc 0.775, validation acc 0.767\n",
      "epoch 59, loss 434.4249, train acc 0.787, validation acc 0.776\n",
      "epoch 60, loss 424.2075, train acc 0.802, validation acc 0.796\n",
      "epoch 61, loss 413.9971, train acc 0.807, validation acc 0.801\n",
      "epoch 62, loss 403.9618, train acc 0.814, validation acc 0.804\n",
      "epoch 63, loss 394.2459, train acc 0.820, validation acc 0.809\n",
      "epoch 64, loss 384.9601, train acc 0.819, validation acc 0.820\n",
      "epoch 65, loss 376.1772, train acc 0.827, validation acc 0.818\n",
      "epoch 66, loss 367.9348, train acc 0.827, validation acc 0.826\n",
      "epoch 67, loss 360.2406, train acc 0.833, validation acc 0.830\n",
      "epoch 68, loss 353.0803, train acc 0.839, validation acc 0.834\n",
      "epoch 69, loss 346.4245, train acc 0.846, validation acc 0.834\n",
      "epoch 70, loss 340.2353, train acc 0.851, validation acc 0.840\n",
      "epoch 71, loss 334.4714, train acc 0.853, validation acc 0.844\n",
      "epoch 72, loss 329.0906, train acc 0.855, validation acc 0.846\n",
      "epoch 73, loss 324.0530, train acc 0.861, validation acc 0.848\n",
      "epoch 74, loss 319.3216, train acc 0.863, validation acc 0.855\n",
      "epoch 75, loss 314.8630, train acc 0.865, validation acc 0.860\n",
      "epoch 76, loss 310.6478, train acc 0.866, validation acc 0.865\n",
      "epoch 77, loss 306.6500, train acc 0.869, validation acc 0.868\n",
      "epoch 78, loss 302.8474, train acc 0.871, validation acc 0.871\n",
      "epoch 79, loss 299.2209, train acc 0.871, validation acc 0.872\n",
      "epoch 80, loss 295.7538, train acc 0.878, validation acc 0.874\n",
      "epoch 81, loss 292.4324, train acc 0.880, validation acc 0.877\n",
      "epoch 82, loss 289.2445, train acc 0.881, validation acc 0.883\n",
      "epoch 83, loss 286.1801, train acc 0.883, validation acc 0.888\n",
      "epoch 84, loss 283.2305, train acc 0.883, validation acc 0.891\n",
      "epoch 85, loss 280.3884, train acc 0.885, validation acc 0.894\n",
      "epoch 86, loss 277.6473, train acc 0.887, validation acc 0.894\n",
      "epoch 87, loss 275.0018, train acc 0.888, validation acc 0.894\n",
      "epoch 88, loss 272.4472, train acc 0.888, validation acc 0.894\n",
      "epoch 89, loss 269.9791, train acc 0.890, validation acc 0.893\n",
      "epoch 90, loss 267.5939, train acc 0.893, validation acc 0.894\n",
      "epoch 91, loss 265.2882, train acc 0.895, validation acc 0.896\n",
      "epoch 92, loss 263.0589, train acc 0.898, validation acc 0.899\n",
      "epoch 93, loss 260.9031, train acc 0.898, validation acc 0.900\n",
      "epoch 94, loss 258.8181, train acc 0.898, validation acc 0.900\n",
      "epoch 95, loss 256.8015, train acc 0.901, validation acc 0.905\n",
      "epoch 96, loss 254.8506, train acc 0.901, validation acc 0.904\n",
      "epoch 97, loss 252.9633, train acc 0.902, validation acc 0.905\n",
      "epoch 98, loss 251.1371, train acc 0.903, validation acc 0.907\n",
      "epoch 99, loss 249.3700, train acc 0.903, validation acc 0.905\n",
      "epoch 100, loss 247.6596, train acc 0.903, validation acc 0.904\n",
      "epoch 101, loss 246.0039, train acc 0.903, validation acc 0.904\n",
      "epoch 102, loss 244.4008, train acc 0.904, validation acc 0.904\n",
      "epoch 103, loss 242.8482, train acc 0.905, validation acc 0.905\n",
      "epoch 104, loss 241.3442, train acc 0.906, validation acc 0.904\n",
      "epoch 105, loss 239.8867, train acc 0.907, validation acc 0.904\n",
      "epoch 106, loss 238.4738, train acc 0.909, validation acc 0.904\n",
      "epoch 107, loss 237.1037, train acc 0.909, validation acc 0.902\n",
      "epoch 108, loss 235.7745, train acc 0.910, validation acc 0.902\n",
      "epoch 109, loss 234.4845, train acc 0.911, validation acc 0.902\n",
      "epoch 110, loss 233.2319, train acc 0.910, validation acc 0.902\n",
      "epoch 111, loss 232.0151, train acc 0.911, validation acc 0.904\n",
      "epoch 112, loss 230.8325, train acc 0.910, validation acc 0.904\n",
      "epoch 113, loss 229.6825, train acc 0.910, validation acc 0.904\n",
      "epoch 114, loss 228.5637, train acc 0.911, validation acc 0.905\n",
      "epoch 115, loss 227.4747, train acc 0.912, validation acc 0.905\n",
      "epoch 116, loss 226.4140, train acc 0.911, validation acc 0.904\n",
      "epoch 117, loss 225.3805, train acc 0.911, validation acc 0.904\n",
      "epoch 118, loss 224.3729, train acc 0.911, validation acc 0.904\n",
      "epoch 119, loss 223.3899, train acc 0.912, validation acc 0.904\n",
      "epoch 120, loss 222.4306, train acc 0.913, validation acc 0.905\n",
      "epoch 121, loss 221.4938, train acc 0.913, validation acc 0.905\n",
      "epoch 122, loss 220.5786, train acc 0.913, validation acc 0.905\n",
      "epoch 123, loss 219.6840, train acc 0.915, validation acc 0.905\n",
      "epoch 124, loss 218.8090, train acc 0.915, validation acc 0.904\n",
      "epoch 125, loss 217.9529, train acc 0.915, validation acc 0.904\n",
      "epoch 126, loss 217.1149, train acc 0.915, validation acc 0.905\n",
      "epoch 127, loss 216.2941, train acc 0.916, validation acc 0.905\n",
      "epoch 128, loss 215.4899, train acc 0.916, validation acc 0.905\n",
      "epoch 129, loss 214.7015, train acc 0.917, validation acc 0.905\n",
      "epoch 130, loss 213.9283, train acc 0.918, validation acc 0.907\n",
      "epoch 131, loss 213.1697, train acc 0.918, validation acc 0.907\n",
      "epoch 132, loss 212.4251, train acc 0.918, validation acc 0.908\n",
      "epoch 133, loss 211.6939, train acc 0.918, validation acc 0.910\n",
      "epoch 134, loss 210.9756, train acc 0.919, validation acc 0.910\n",
      "epoch 135, loss 210.2697, train acc 0.919, validation acc 0.910\n",
      "epoch 136, loss 209.5757, train acc 0.918, validation acc 0.910\n",
      "epoch 137, loss 208.8932, train acc 0.918, validation acc 0.911\n",
      "epoch 138, loss 208.2217, train acc 0.918, validation acc 0.911\n",
      "epoch 139, loss 207.5607, train acc 0.918, validation acc 0.911\n",
      "epoch 140, loss 206.9100, train acc 0.919, validation acc 0.911\n",
      "epoch 141, loss 206.2691, train acc 0.918, validation acc 0.911\n",
      "epoch 142, loss 205.6376, train acc 0.917, validation acc 0.911\n",
      "epoch 143, loss 205.0153, train acc 0.917, validation acc 0.911\n",
      "epoch 144, loss 204.4018, train acc 0.917, validation acc 0.911\n",
      "epoch 145, loss 203.7968, train acc 0.917, validation acc 0.911\n",
      "epoch 146, loss 203.1999, train acc 0.917, validation acc 0.911\n",
      "epoch 147, loss 202.6110, train acc 0.918, validation acc 0.911\n",
      "epoch 148, loss 202.0298, train acc 0.918, validation acc 0.911\n",
      "epoch 149, loss 201.4559, train acc 0.919, validation acc 0.911\n",
      "epoch 150, loss 200.8892, train acc 0.920, validation acc 0.911\n",
      "epoch 151, loss 200.3294, train acc 0.921, validation acc 0.911\n",
      "epoch 152, loss 199.7764, train acc 0.921, validation acc 0.911\n",
      "epoch 153, loss 199.2299, train acc 0.922, validation acc 0.911\n",
      "epoch 154, loss 198.6896, train acc 0.922, validation acc 0.911\n",
      "epoch 155, loss 198.1556, train acc 0.922, validation acc 0.910\n",
      "epoch 156, loss 197.6274, train acc 0.922, validation acc 0.910\n",
      "epoch 157, loss 197.1051, train acc 0.922, validation acc 0.910\n",
      "epoch 158, loss 196.5884, train acc 0.922, validation acc 0.910\n",
      "epoch 159, loss 196.0772, train acc 0.922, validation acc 0.908\n",
      "epoch 160, loss 195.5714, train acc 0.922, validation acc 0.908\n",
      "epoch 161, loss 195.0708, train acc 0.923, validation acc 0.908\n",
      "epoch 162, loss 194.5752, train acc 0.923, validation acc 0.908\n",
      "epoch 163, loss 194.0847, train acc 0.924, validation acc 0.908\n",
      "epoch 164, loss 193.5990, train acc 0.924, validation acc 0.908\n",
      "epoch 165, loss 193.1181, train acc 0.924, validation acc 0.908\n",
      "epoch 166, loss 192.6419, train acc 0.926, validation acc 0.908\n",
      "epoch 167, loss 192.1702, train acc 0.927, validation acc 0.907\n",
      "epoch 168, loss 191.7030, train acc 0.928, validation acc 0.905\n",
      "epoch 169, loss 191.2402, train acc 0.929, validation acc 0.905\n",
      "epoch 170, loss 190.7817, train acc 0.929, validation acc 0.905\n",
      "epoch 171, loss 190.3275, train acc 0.929, validation acc 0.904\n",
      "epoch 172, loss 189.8774, train acc 0.929, validation acc 0.905\n",
      "epoch 173, loss 189.4314, train acc 0.929, validation acc 0.905\n",
      "epoch 174, loss 188.9895, train acc 0.929, validation acc 0.905\n",
      "epoch 175, loss 188.5515, train acc 0.929, validation acc 0.905\n",
      "epoch 176, loss 188.1173, train acc 0.930, validation acc 0.905\n",
      "epoch 177, loss 187.6871, train acc 0.931, validation acc 0.904\n",
      "epoch 178, loss 187.2605, train acc 0.931, validation acc 0.904\n",
      "epoch 179, loss 186.8377, train acc 0.931, validation acc 0.904\n",
      "epoch 180, loss 186.4186, train acc 0.931, validation acc 0.904\n",
      "epoch 181, loss 186.0030, train acc 0.931, validation acc 0.904\n",
      "epoch 182, loss 185.5910, train acc 0.931, validation acc 0.904\n",
      "epoch 183, loss 185.1825, train acc 0.931, validation acc 0.904\n",
      "epoch 184, loss 184.7775, train acc 0.931, validation acc 0.904\n",
      "epoch 185, loss 184.3758, train acc 0.931, validation acc 0.904\n",
      "epoch 186, loss 183.9775, train acc 0.931, validation acc 0.904\n",
      "epoch 187, loss 183.5824, train acc 0.931, validation acc 0.904\n",
      "epoch 188, loss 183.1906, train acc 0.931, validation acc 0.904\n",
      "epoch 189, loss 182.8020, train acc 0.931, validation acc 0.904\n",
      "epoch 190, loss 182.4165, train acc 0.931, validation acc 0.904\n",
      "epoch 191, loss 182.0342, train acc 0.932, validation acc 0.904\n",
      "epoch 192, loss 181.6549, train acc 0.932, validation acc 0.904\n",
      "epoch 193, loss 181.2786, train acc 0.932, validation acc 0.904\n",
      "epoch 194, loss 180.9052, train acc 0.932, validation acc 0.904\n",
      "epoch 195, loss 180.5348, train acc 0.932, validation acc 0.904\n",
      "epoch 196, loss 180.1673, train acc 0.932, validation acc 0.904\n",
      "epoch 197, loss 179.8026, train acc 0.932, validation acc 0.905\n",
      "epoch 198, loss 179.4407, train acc 0.932, validation acc 0.905\n",
      "epoch 199, loss 179.0816, train acc 0.932, validation acc 0.905\n",
      "epoch 200, loss 178.7252, train acc 0.932, validation acc 0.905\n",
      "epoch 201, loss 178.3715, train acc 0.932, validation acc 0.905\n",
      "epoch 202, loss 178.0204, train acc 0.932, validation acc 0.905\n",
      "epoch 203, loss 177.6719, train acc 0.933, validation acc 0.905\n",
      "epoch 204, loss 177.3260, train acc 0.933, validation acc 0.905\n",
      "epoch 205, loss 176.9827, train acc 0.933, validation acc 0.905\n",
      "epoch 206, loss 176.6418, train acc 0.935, validation acc 0.905\n",
      "epoch 207, loss 176.3034, train acc 0.935, validation acc 0.905\n",
      "epoch 208, loss 175.9674, train acc 0.936, validation acc 0.907\n",
      "epoch 209, loss 175.6339, train acc 0.937, validation acc 0.907\n",
      "epoch 210, loss 175.3027, train acc 0.937, validation acc 0.907\n",
      "epoch 211, loss 174.9739, train acc 0.937, validation acc 0.907\n",
      "epoch 212, loss 174.6474, train acc 0.937, validation acc 0.907\n",
      "epoch 213, loss 174.3231, train acc 0.937, validation acc 0.905\n",
      "epoch 214, loss 174.0011, train acc 0.937, validation acc 0.905\n",
      "epoch 215, loss 173.6814, train acc 0.937, validation acc 0.905\n",
      "epoch 216, loss 173.3638, train acc 0.938, validation acc 0.905\n",
      "epoch 217, loss 173.0484, train acc 0.938, validation acc 0.905\n",
      "epoch 218, loss 172.7351, train acc 0.938, validation acc 0.905\n",
      "epoch 219, loss 172.4240, train acc 0.938, validation acc 0.905\n",
      "epoch 220, loss 172.1149, train acc 0.938, validation acc 0.905\n",
      "epoch 221, loss 171.8079, train acc 0.939, validation acc 0.905\n",
      "epoch 222, loss 171.5029, train acc 0.939, validation acc 0.905\n",
      "epoch 223, loss 171.1999, train acc 0.939, validation acc 0.905\n",
      "epoch 224, loss 170.8989, train acc 0.939, validation acc 0.905\n",
      "epoch 225, loss 170.5999, train acc 0.939, validation acc 0.905\n",
      "epoch 226, loss 170.3027, train acc 0.939, validation acc 0.905\n",
      "epoch 227, loss 170.0075, train acc 0.939, validation acc 0.905\n",
      "epoch 228, loss 169.7141, train acc 0.939, validation acc 0.905\n",
      "epoch 229, loss 169.4226, train acc 0.939, validation acc 0.905\n",
      "epoch 230, loss 169.1328, train acc 0.939, validation acc 0.907\n",
      "epoch 231, loss 168.8449, train acc 0.940, validation acc 0.907\n",
      "epoch 232, loss 168.5587, train acc 0.940, validation acc 0.907\n",
      "epoch 233, loss 168.2743, train acc 0.940, validation acc 0.905\n",
      "epoch 234, loss 167.9916, train acc 0.940, validation acc 0.905\n",
      "epoch 235, loss 167.7105, train acc 0.940, validation acc 0.905\n",
      "epoch 236, loss 167.4311, train acc 0.940, validation acc 0.905\n",
      "epoch 237, loss 167.1534, train acc 0.940, validation acc 0.905\n",
      "epoch 238, loss 166.8772, train acc 0.940, validation acc 0.905\n",
      "epoch 239, loss 166.6026, train acc 0.940, validation acc 0.905\n",
      "epoch 240, loss 166.3296, train acc 0.940, validation acc 0.905\n",
      "epoch 241, loss 166.0580, train acc 0.941, validation acc 0.905\n",
      "epoch 242, loss 165.7880, train acc 0.941, validation acc 0.905\n",
      "epoch 243, loss 165.5195, train acc 0.941, validation acc 0.905\n",
      "epoch 244, loss 165.2524, train acc 0.941, validation acc 0.907\n",
      "epoch 245, loss 164.9867, train acc 0.941, validation acc 0.907\n",
      "epoch 246, loss 164.7224, train acc 0.941, validation acc 0.907\n",
      "epoch 247, loss 164.4595, train acc 0.942, validation acc 0.905\n",
      "epoch 248, loss 164.1980, train acc 0.943, validation acc 0.907\n",
      "epoch 249, loss 163.9378, train acc 0.943, validation acc 0.907\n",
      "epoch 250, loss 163.6788, train acc 0.943, validation acc 0.907\n",
      "epoch 251, loss 163.4212, train acc 0.943, validation acc 0.907\n",
      "epoch 252, loss 163.1648, train acc 0.943, validation acc 0.907\n",
      "epoch 253, loss 162.9096, train acc 0.943, validation acc 0.907\n",
      "epoch 254, loss 162.6557, train acc 0.943, validation acc 0.907\n",
      "epoch 255, loss 162.4029, train acc 0.943, validation acc 0.907\n",
      "epoch 256, loss 162.1513, train acc 0.943, validation acc 0.907\n",
      "epoch 257, loss 161.9008, train acc 0.944, validation acc 0.907\n",
      "epoch 258, loss 161.6515, train acc 0.944, validation acc 0.907\n",
      "epoch 259, loss 161.4033, train acc 0.944, validation acc 0.907\n",
      "epoch 260, loss 161.1561, train acc 0.945, validation acc 0.907\n",
      "epoch 261, loss 160.9101, train acc 0.945, validation acc 0.907\n",
      "epoch 262, loss 160.6651, train acc 0.945, validation acc 0.907\n",
      "epoch 263, loss 160.4211, train acc 0.945, validation acc 0.908\n",
      "epoch 264, loss 160.1781, train acc 0.944, validation acc 0.910\n",
      "epoch 265, loss 159.9361, train acc 0.944, validation acc 0.910\n",
      "epoch 266, loss 159.6951, train acc 0.944, validation acc 0.910\n",
      "epoch 267, loss 159.4550, train acc 0.944, validation acc 0.910\n",
      "epoch 268, loss 159.2159, train acc 0.944, validation acc 0.910\n",
      "epoch 269, loss 158.9778, train acc 0.944, validation acc 0.910\n",
      "epoch 270, loss 158.7405, train acc 0.944, validation acc 0.910\n",
      "epoch 271, loss 158.5042, train acc 0.943, validation acc 0.910\n",
      "epoch 272, loss 158.2688, train acc 0.943, validation acc 0.910\n",
      "epoch 273, loss 158.0342, train acc 0.943, validation acc 0.910\n",
      "epoch 274, loss 157.8005, train acc 0.943, validation acc 0.910\n",
      "epoch 275, loss 157.5677, train acc 0.943, validation acc 0.910\n",
      "epoch 276, loss 157.3357, train acc 0.943, validation acc 0.911\n",
      "epoch 277, loss 157.1045, train acc 0.943, validation acc 0.911\n",
      "epoch 278, loss 156.8742, train acc 0.943, validation acc 0.911\n",
      "epoch 279, loss 156.6447, train acc 0.943, validation acc 0.911\n",
      "epoch 280, loss 156.4160, train acc 0.943, validation acc 0.911\n",
      "epoch 281, loss 156.1881, train acc 0.944, validation acc 0.911\n",
      "epoch 282, loss 155.9610, train acc 0.944, validation acc 0.910\n",
      "epoch 283, loss 155.7347, train acc 0.944, validation acc 0.910\n",
      "epoch 284, loss 155.5092, train acc 0.943, validation acc 0.910\n",
      "epoch 285, loss 155.2845, train acc 0.943, validation acc 0.910\n",
      "epoch 286, loss 155.0606, train acc 0.943, validation acc 0.910\n",
      "epoch 287, loss 154.8374, train acc 0.943, validation acc 0.910\n",
      "epoch 288, loss 154.6150, train acc 0.943, validation acc 0.910\n",
      "epoch 289, loss 154.3934, train acc 0.943, validation acc 0.910\n",
      "epoch 290, loss 154.1725, train acc 0.943, validation acc 0.910\n",
      "epoch 291, loss 153.9524, train acc 0.943, validation acc 0.910\n",
      "epoch 292, loss 153.7330, train acc 0.943, validation acc 0.910\n",
      "epoch 293, loss 153.5144, train acc 0.943, validation acc 0.910\n",
      "epoch 294, loss 153.2966, train acc 0.943, validation acc 0.910\n",
      "epoch 295, loss 153.0796, train acc 0.943, validation acc 0.910\n",
      "epoch 296, loss 152.8633, train acc 0.943, validation acc 0.910\n",
      "epoch 297, loss 152.6477, train acc 0.943, validation acc 0.910\n",
      "epoch 298, loss 152.4330, train acc 0.943, validation acc 0.910\n",
      "epoch 299, loss 152.2190, train acc 0.943, validation acc 0.910\n",
      "epoch 300, loss 152.0057, train acc 0.943, validation acc 0.910\n",
      "epoch 301, loss 151.7933, train acc 0.943, validation acc 0.910\n",
      "epoch 302, loss 151.5816, train acc 0.943, validation acc 0.910\n",
      "epoch 303, loss 151.3706, train acc 0.943, validation acc 0.910\n",
      "epoch 304, loss 151.1605, train acc 0.943, validation acc 0.910\n",
      "epoch 305, loss 150.9511, train acc 0.943, validation acc 0.910\n",
      "epoch 306, loss 150.7425, train acc 0.943, validation acc 0.911\n",
      "epoch 307, loss 150.5347, train acc 0.943, validation acc 0.911\n",
      "epoch 308, loss 150.3277, train acc 0.943, validation acc 0.911\n",
      "epoch 309, loss 150.1214, train acc 0.943, validation acc 0.911\n",
      "epoch 310, loss 149.9160, train acc 0.943, validation acc 0.911\n",
      "epoch 311, loss 149.7113, train acc 0.944, validation acc 0.911\n",
      "epoch 312, loss 149.5074, train acc 0.945, validation acc 0.911\n",
      "epoch 313, loss 149.3044, train acc 0.945, validation acc 0.911\n",
      "epoch 314, loss 149.1021, train acc 0.945, validation acc 0.910\n",
      "epoch 315, loss 148.9007, train acc 0.945, validation acc 0.910\n",
      "epoch 316, loss 148.7000, train acc 0.946, validation acc 0.910\n",
      "epoch 317, loss 148.5002, train acc 0.946, validation acc 0.910\n",
      "epoch 318, loss 148.3011, train acc 0.947, validation acc 0.910\n",
      "epoch 319, loss 148.1029, train acc 0.947, validation acc 0.910\n",
      "epoch 320, loss 147.9055, train acc 0.947, validation acc 0.910\n",
      "epoch 321, loss 147.7088, train acc 0.947, validation acc 0.910\n",
      "epoch 322, loss 147.5131, train acc 0.947, validation acc 0.910\n",
      "epoch 323, loss 147.3181, train acc 0.948, validation acc 0.910\n",
      "epoch 324, loss 147.1239, train acc 0.948, validation acc 0.910\n",
      "epoch 325, loss 146.9306, train acc 0.948, validation acc 0.910\n",
      "epoch 326, loss 146.7381, train acc 0.949, validation acc 0.910\n",
      "epoch 327, loss 146.5464, train acc 0.949, validation acc 0.910\n",
      "epoch 328, loss 146.3555, train acc 0.949, validation acc 0.910\n",
      "epoch 329, loss 146.1654, train acc 0.950, validation acc 0.910\n",
      "epoch 330, loss 145.9762, train acc 0.950, validation acc 0.910\n",
      "epoch 331, loss 145.7877, train acc 0.950, validation acc 0.910\n",
      "epoch 332, loss 145.6001, train acc 0.951, validation acc 0.910\n",
      "epoch 333, loss 145.4133, train acc 0.951, validation acc 0.910\n",
      "epoch 334, loss 145.2274, train acc 0.951, validation acc 0.910\n",
      "epoch 335, loss 145.0422, train acc 0.951, validation acc 0.910\n",
      "epoch 336, loss 144.8579, train acc 0.951, validation acc 0.910\n",
      "epoch 337, loss 144.6744, train acc 0.951, validation acc 0.910\n",
      "epoch 338, loss 144.4917, train acc 0.951, validation acc 0.910\n",
      "epoch 339, loss 144.3098, train acc 0.952, validation acc 0.910\n",
      "epoch 340, loss 144.1288, train acc 0.952, validation acc 0.910\n",
      "epoch 341, loss 143.9485, train acc 0.952, validation acc 0.910\n",
      "epoch 342, loss 143.7691, train acc 0.952, validation acc 0.910\n",
      "epoch 343, loss 143.5905, train acc 0.952, validation acc 0.910\n",
      "epoch 344, loss 143.4127, train acc 0.953, validation acc 0.911\n",
      "epoch 345, loss 143.2357, train acc 0.953, validation acc 0.911\n",
      "epoch 346, loss 143.0595, train acc 0.953, validation acc 0.911\n",
      "epoch 347, loss 142.8841, train acc 0.953, validation acc 0.911\n",
      "epoch 348, loss 142.7096, train acc 0.953, validation acc 0.911\n",
      "epoch 349, loss 142.5358, train acc 0.953, validation acc 0.911\n",
      "epoch 350, loss 142.3629, train acc 0.953, validation acc 0.911\n",
      "epoch 351, loss 142.1907, train acc 0.953, validation acc 0.911\n",
      "epoch 352, loss 142.0194, train acc 0.954, validation acc 0.911\n",
      "epoch 353, loss 141.8489, train acc 0.954, validation acc 0.911\n",
      "epoch 354, loss 141.6791, train acc 0.954, validation acc 0.911\n",
      "epoch 355, loss 141.5102, train acc 0.954, validation acc 0.911\n",
      "epoch 356, loss 141.3420, train acc 0.954, validation acc 0.911\n",
      "epoch 357, loss 141.1747, train acc 0.954, validation acc 0.911\n",
      "epoch 358, loss 141.0081, train acc 0.954, validation acc 0.911\n",
      "epoch 359, loss 140.8424, train acc 0.954, validation acc 0.913\n",
      "epoch 360, loss 140.6774, train acc 0.954, validation acc 0.913\n",
      "epoch 361, loss 140.5132, train acc 0.954, validation acc 0.913\n",
      "epoch 362, loss 140.3499, train acc 0.954, validation acc 0.913\n",
      "epoch 363, loss 140.1873, train acc 0.954, validation acc 0.913\n",
      "epoch 364, loss 140.0254, train acc 0.954, validation acc 0.913\n",
      "epoch 365, loss 139.8644, train acc 0.954, validation acc 0.913\n",
      "epoch 366, loss 139.7041, train acc 0.955, validation acc 0.913\n",
      "epoch 367, loss 139.5446, train acc 0.955, validation acc 0.913\n",
      "epoch 368, loss 139.3859, train acc 0.955, validation acc 0.913\n",
      "epoch 369, loss 139.2279, train acc 0.955, validation acc 0.913\n",
      "epoch 370, loss 139.0707, train acc 0.955, validation acc 0.913\n",
      "epoch 371, loss 138.9143, train acc 0.955, validation acc 0.913\n",
      "epoch 372, loss 138.7586, train acc 0.955, validation acc 0.913\n",
      "epoch 373, loss 138.6037, train acc 0.955, validation acc 0.913\n",
      "epoch 374, loss 138.4495, train acc 0.955, validation acc 0.913\n",
      "epoch 375, loss 138.2961, train acc 0.955, validation acc 0.913\n",
      "epoch 376, loss 138.1434, train acc 0.955, validation acc 0.911\n",
      "epoch 377, loss 137.9914, train acc 0.955, validation acc 0.911\n",
      "epoch 378, loss 137.8402, train acc 0.955, validation acc 0.911\n",
      "epoch 379, loss 137.6897, train acc 0.955, validation acc 0.911\n",
      "epoch 380, loss 137.5399, train acc 0.955, validation acc 0.911\n",
      "epoch 381, loss 137.3909, train acc 0.955, validation acc 0.911\n",
      "epoch 382, loss 137.2425, train acc 0.955, validation acc 0.911\n",
      "epoch 383, loss 137.0949, train acc 0.955, validation acc 0.911\n",
      "epoch 384, loss 136.9479, train acc 0.955, validation acc 0.911\n",
      "epoch 385, loss 136.8017, train acc 0.955, validation acc 0.913\n",
      "epoch 386, loss 136.6561, train acc 0.956, validation acc 0.913\n",
      "epoch 387, loss 136.5112, train acc 0.956, validation acc 0.913\n",
      "epoch 388, loss 136.3670, train acc 0.956, validation acc 0.913\n",
      "epoch 389, loss 136.2234, train acc 0.956, validation acc 0.913\n",
      "epoch 390, loss 136.0806, train acc 0.956, validation acc 0.913\n",
      "epoch 391, loss 135.9383, train acc 0.956, validation acc 0.913\n",
      "epoch 392, loss 135.7967, train acc 0.956, validation acc 0.913\n",
      "epoch 393, loss 135.6558, train acc 0.956, validation acc 0.913\n",
      "epoch 394, loss 135.5155, train acc 0.956, validation acc 0.913\n",
      "epoch 395, loss 135.3758, train acc 0.955, validation acc 0.913\n",
      "epoch 396, loss 135.2368, train acc 0.955, validation acc 0.913\n",
      "epoch 397, loss 135.0984, train acc 0.956, validation acc 0.914\n",
      "epoch 398, loss 134.9605, train acc 0.956, validation acc 0.914\n",
      "epoch 399, loss 134.8233, train acc 0.956, validation acc 0.914\n",
      "epoch 400, loss 134.6867, train acc 0.956, validation acc 0.914\n",
      "epoch 401, loss 134.5507, train acc 0.956, validation acc 0.914\n",
      "epoch 402, loss 134.4152, train acc 0.956, validation acc 0.914\n",
      "epoch 403, loss 134.2803, train acc 0.956, validation acc 0.914\n",
      "epoch 404, loss 134.1460, train acc 0.956, validation acc 0.914\n",
      "epoch 405, loss 134.0123, train acc 0.957, validation acc 0.914\n",
      "epoch 406, loss 133.8791, train acc 0.957, validation acc 0.914\n",
      "epoch 407, loss 133.7464, train acc 0.957, validation acc 0.914\n",
      "epoch 408, loss 133.6143, train acc 0.957, validation acc 0.914\n",
      "epoch 409, loss 133.4828, train acc 0.957, validation acc 0.914\n",
      "epoch 410, loss 133.3517, train acc 0.957, validation acc 0.914\n",
      "epoch 411, loss 133.2212, train acc 0.957, validation acc 0.914\n",
      "epoch 412, loss 133.0912, train acc 0.957, validation acc 0.914\n",
      "epoch 413, loss 132.9617, train acc 0.957, validation acc 0.914\n",
      "epoch 414, loss 132.8327, train acc 0.957, validation acc 0.914\n",
      "epoch 415, loss 132.7042, train acc 0.957, validation acc 0.914\n",
      "epoch 416, loss 132.5762, train acc 0.957, validation acc 0.914\n",
      "epoch 417, loss 132.4486, train acc 0.957, validation acc 0.914\n",
      "epoch 418, loss 132.3216, train acc 0.957, validation acc 0.914\n",
      "epoch 419, loss 132.1950, train acc 0.957, validation acc 0.914\n",
      "epoch 420, loss 132.0689, train acc 0.957, validation acc 0.914\n",
      "epoch 421, loss 131.9432, train acc 0.957, validation acc 0.914\n",
      "epoch 422, loss 131.8180, train acc 0.957, validation acc 0.914\n",
      "epoch 423, loss 131.6932, train acc 0.957, validation acc 0.913\n",
      "epoch 424, loss 131.5689, train acc 0.957, validation acc 0.911\n",
      "epoch 425, loss 131.4450, train acc 0.957, validation acc 0.913\n",
      "epoch 426, loss 131.3215, train acc 0.957, validation acc 0.913\n",
      "epoch 427, loss 131.1984, train acc 0.957, validation acc 0.913\n",
      "epoch 428, loss 131.0758, train acc 0.957, validation acc 0.913\n",
      "epoch 429, loss 130.9535, train acc 0.957, validation acc 0.913\n",
      "epoch 430, loss 130.8317, train acc 0.957, validation acc 0.913\n",
      "epoch 431, loss 130.7102, train acc 0.957, validation acc 0.913\n",
      "epoch 432, loss 130.5892, train acc 0.957, validation acc 0.913\n",
      "epoch 433, loss 130.4685, train acc 0.957, validation acc 0.913\n",
      "epoch 434, loss 130.3482, train acc 0.957, validation acc 0.913\n",
      "epoch 435, loss 130.2283, train acc 0.957, validation acc 0.913\n",
      "epoch 436, loss 130.1087, train acc 0.957, validation acc 0.913\n",
      "epoch 437, loss 129.9895, train acc 0.957, validation acc 0.913\n",
      "epoch 438, loss 129.8706, train acc 0.957, validation acc 0.913\n",
      "epoch 439, loss 129.7521, train acc 0.957, validation acc 0.911\n",
      "epoch 440, loss 129.6340, train acc 0.957, validation acc 0.911\n",
      "epoch 441, loss 129.5161, train acc 0.958, validation acc 0.911\n",
      "epoch 442, loss 129.3986, train acc 0.959, validation acc 0.911\n",
      "epoch 443, loss 129.2815, train acc 0.959, validation acc 0.911\n",
      "epoch 444, loss 129.1646, train acc 0.959, validation acc 0.911\n",
      "epoch 445, loss 129.0481, train acc 0.959, validation acc 0.911\n",
      "epoch 446, loss 128.9319, train acc 0.959, validation acc 0.911\n",
      "epoch 447, loss 128.8159, train acc 0.959, validation acc 0.908\n",
      "epoch 448, loss 128.7003, train acc 0.959, validation acc 0.908\n",
      "epoch 449, loss 128.5849, train acc 0.959, validation acc 0.908\n",
      "epoch 450, loss 128.4699, train acc 0.959, validation acc 0.908\n",
      "epoch 451, loss 128.3551, train acc 0.959, validation acc 0.910\n",
      "epoch 452, loss 128.2406, train acc 0.959, validation acc 0.910\n",
      "epoch 453, loss 128.1264, train acc 0.960, validation acc 0.910\n",
      "epoch 454, loss 128.0124, train acc 0.960, validation acc 0.910\n",
      "epoch 455, loss 127.8987, train acc 0.960, validation acc 0.910\n",
      "epoch 456, loss 127.7852, train acc 0.960, validation acc 0.910\n",
      "epoch 457, loss 127.6720, train acc 0.960, validation acc 0.910\n",
      "epoch 458, loss 127.5590, train acc 0.960, validation acc 0.910\n",
      "epoch 459, loss 127.4463, train acc 0.960, validation acc 0.910\n",
      "epoch 460, loss 127.3338, train acc 0.960, validation acc 0.910\n",
      "epoch 461, loss 127.2215, train acc 0.961, validation acc 0.910\n",
      "epoch 462, loss 127.1094, train acc 0.961, validation acc 0.910\n",
      "epoch 463, loss 126.9976, train acc 0.961, validation acc 0.910\n",
      "epoch 464, loss 126.8859, train acc 0.961, validation acc 0.910\n",
      "epoch 465, loss 126.7745, train acc 0.961, validation acc 0.910\n",
      "epoch 466, loss 126.6633, train acc 0.961, validation acc 0.910\n",
      "epoch 467, loss 126.5522, train acc 0.961, validation acc 0.910\n",
      "epoch 468, loss 126.4414, train acc 0.961, validation acc 0.910\n",
      "epoch 469, loss 126.3307, train acc 0.961, validation acc 0.910\n",
      "epoch 470, loss 126.2203, train acc 0.961, validation acc 0.910\n",
      "epoch 471, loss 126.1100, train acc 0.961, validation acc 0.910\n",
      "epoch 472, loss 125.9999, train acc 0.961, validation acc 0.910\n",
      "epoch 473, loss 125.8900, train acc 0.961, validation acc 0.910\n",
      "epoch 474, loss 125.7802, train acc 0.961, validation acc 0.910\n",
      "epoch 475, loss 125.6706, train acc 0.961, validation acc 0.910\n",
      "epoch 476, loss 125.5612, train acc 0.961, validation acc 0.910\n",
      "epoch 477, loss 125.4519, train acc 0.961, validation acc 0.910\n",
      "epoch 478, loss 125.3428, train acc 0.961, validation acc 0.910\n",
      "epoch 479, loss 125.2339, train acc 0.961, validation acc 0.910\n",
      "epoch 480, loss 125.1251, train acc 0.961, validation acc 0.910\n",
      "epoch 481, loss 125.0165, train acc 0.961, validation acc 0.910\n",
      "epoch 482, loss 124.9080, train acc 0.961, validation acc 0.910\n",
      "epoch 483, loss 124.7997, train acc 0.961, validation acc 0.910\n",
      "epoch 484, loss 124.6916, train acc 0.961, validation acc 0.910\n",
      "epoch 485, loss 124.5836, train acc 0.961, validation acc 0.910\n",
      "epoch 486, loss 124.4757, train acc 0.961, validation acc 0.910\n",
      "epoch 487, loss 124.3680, train acc 0.961, validation acc 0.910\n",
      "epoch 488, loss 124.2605, train acc 0.961, validation acc 0.910\n",
      "epoch 489, loss 124.1531, train acc 0.961, validation acc 0.910\n",
      "epoch 490, loss 124.0459, train acc 0.961, validation acc 0.910\n",
      "epoch 491, loss 123.9389, train acc 0.960, validation acc 0.910\n",
      "epoch 492, loss 123.8320, train acc 0.960, validation acc 0.910\n",
      "epoch 493, loss 123.7253, train acc 0.961, validation acc 0.910\n",
      "epoch 494, loss 123.6187, train acc 0.961, validation acc 0.910\n",
      "epoch 495, loss 123.5124, train acc 0.961, validation acc 0.910\n",
      "epoch 496, loss 123.4062, train acc 0.961, validation acc 0.910\n",
      "epoch 497, loss 123.3002, train acc 0.961, validation acc 0.910\n",
      "epoch 498, loss 123.1944, train acc 0.961, validation acc 0.910\n",
      "epoch 499, loss 123.0887, train acc 0.961, validation acc 0.910\n",
      "epoch 500, loss 122.9833, train acc 0.961, validation acc 0.910\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "# enter code here to train Model1\n",
    "# TODO: set your desired hidden size, learning rate, number of iterations and batch size\n",
    "\n",
    "model = NN(learning_rate=0.01, n_iters=500, batch_size=100, hidden_size=32, device=device)\n",
    "model.train(train_X,train_Y,test_X,test_Y)\n",
    "\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "6ab4eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      " [[131.  30.]\n",
      " [ 28. 454.]] \n",
      "Accuracy:  0.9097978227060654 \n",
      "Precision:  0.8238993710691824 \n",
      "Recall:  0.8136645962732919 \n",
      "F1 score:  0.8187500000000001\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix, accuracy,precision,recall,F1_Score = evaluator(test_Y.cpu().numpy(),model.predict(test_X).cpu().numpy())\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix , \"\\nAccuracy: \", accuracy , \"\\nPrecision: \", precision , \"\\nRecall: \", recall , \"\\nF1 score: \", F1_Score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "db7fc976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model.history['train_acc'], label='train_acc')\n",
    "plt.plot(model.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "0660c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_test = pd.read_csv('test.csv')\n",
    "#ttest_x = list(df_test['description_x'].values)\n",
    "#ttest_x_1 = list(df_test['description_y'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "86a35432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#ttest_X = tfidf.transform(ttest_x).toarray()\n",
    "\n",
    "#ttest_X_1 = tfidf.transform(ttest_x_1).toarray()\n",
    "#ttest_X = np.concatenate([ttest_X,ttest_X_1],axis=1)\n",
    "#ttest_X = torch.tensor(ttest_X,dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "bebd0600",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "bebd0600",
    "outputId": "4cf59edb-b5bd-417c-913f-78b5a793189f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n",
       "        0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.predict(ttest_X)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "112ddf535cc8ebb075627ebeef1cd2f0be8e7f6d4abd0e24075c6413308f53c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
